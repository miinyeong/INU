{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOQMS54kEkrq"
      },
      "source": [
        "# 데이터사이언스 (0010085001)\n",
        "\n",
        "## Exercise 13: Logistic Regression\n",
        "\n",
        "In this excercise, we will implement the logistic regression algorithm.\n",
        "\n",
        "* Logistic regression aims to estimate the parameters of a logistic model (the coefficient in the linear combination)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGURm8ewQI-_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8AQY_tZQQRo"
      },
      "outputs": [],
      "source": [
        "# Half moon 데이터 생성 (linear classification 과 동일)\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
        "\n",
        "# numpy 시드 고정 -> 난수의 생성 패턴을 동일하게 관리\n",
        "np.random.seed(0)\n",
        "\n",
        "# 데이터 생성\n",
        "X, y = sklearn.datasets.make_moons(100, noise=0.1)\n",
        "\n",
        "# 데이터 크기 및 샘플 확인\n",
        "print(X[:5])\n",
        "print(y[:5])\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 시각화\n",
        "\n",
        "plt.scatter(X[:, 0], X[:,1], \n",
        "            s=40, \n",
        "            c=y,\n",
        "            cmap=plt.cm.Spectral)\n",
        "\n",
        "plt.title(\"Half moon dataset\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QuGZCjiEUlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic (sigmoid) 함수 정의\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Logistic_function\n",
        "\n",
        "def logistic_function(x):\n",
        "\n",
        "    return 1/ (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "bs93NXauCIiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic 함수 테스트\n",
        "\n",
        "output = logistic_function (0)\n",
        "print(output)\n",
        "\n",
        "output = logistic_function (10)\n",
        "print(output)\n",
        "\n",
        "output = logistic_function (-10)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "m_2fx6OTCIgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic 함수 그리기\n",
        "\n",
        "x_hyperplane = np.linspace(-10, 10, 100)\n",
        "y_hyperplane = logistic_function(x_hyperplane)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x_hyperplane, y_hyperplane)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0yKj421ITQbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error function (cost function; in-sample error) 함수 정의\n",
        "\n",
        "def compute_cost(theta, x, y_true):\n",
        "    # 데이터의 수\n",
        "    N = len(y_true)\n",
        "\n",
        "    # signal 을 logistic function 에 넣어 예측값 (예측 확률) 계산\n",
        "    y_pred = logistic_function(np.dot(x , theta))\n",
        "\n",
        "    # cross-entropy 에러 계산\n",
        "    # p * log(1 / q) + (1 - p) * log(1 / (1 - q)) ... cross entropy 의 정의\n",
        "    # error = (y_true * np.log(1 / y_pred)) - ((1 - y_true) * np.log(1 / (1 - y_pred)))\n",
        "    error = (y_true * np.log(y_pred)) + ((1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    cost = -1 / N * sum(error)\n",
        "    \n",
        "    gradient = 1 / N * np.dot(x.transpose(), (y_pred - y_true))\n",
        "\n",
        "    return cost[0] , gradient"
      ],
      "metadata": {
        "id": "N5fYPfzaCIeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost 및 gradient 초기화\n",
        "\n",
        "# 데이터 정규화\n",
        "mean_X = np.mean(X, axis=0)\n",
        "std_X = np.std(X, axis=0)\n",
        "normalized_X = (X - mean_X) / std_X\n",
        "\n",
        "rows = X.shape[0]\n",
        "cols = X.shape[1]\n",
        "\n",
        "# oneVector 를 추가하는 과정 (linear classification 과 동일)\n",
        "X_ = np.append(np.ones((rows, 1)), normalized_X, axis=1)\n",
        "y = y.reshape(rows, 1)\n",
        "\n",
        "# 가중치 (theta 초기화)\n",
        "theta_init = np.zeros((cols + 1, 1))\n",
        "\n",
        "# 0-th step 에서 cost 및 gradient 계산\n",
        "cost, gradient = compute_cost(theta_init, X_, y)\n",
        "\n",
        "print(\"Cost at initialization\", cost)\n",
        "print(\"Gradient at initialization:\", gradient)"
      ],
      "metadata": {
        "id": "CFgGpH4VCIb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent 함수 정의\n",
        "\n",
        "def gradient_descent(x, y, theta, learning_rate, iterations):\n",
        "    costs = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # 주어진 데이터에 대한 cost 와 gradient 값 계산\n",
        "        cost, gradient = compute_cost(theta, x, y)\n",
        "\n",
        "        # w(t+1) <- w(t) - learning_rate * gradient\n",
        "        theta -= (learning_rate * gradient)\n",
        "        \n",
        "        # cost 에 대한 plot 을 그리기 위해 계산한 값을 costs 에 저장\n",
        "        costs.append(cost)\n",
        "        \n",
        "    return theta, costs"
      ],
      "metadata": {
        "id": "xW87zfEvCIZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent 알고리즘 수행\n",
        "\n",
        "# learning_rate: 1, iterstions: 200\n",
        "theta, costs = gradient_descent(X_, y, theta_init, 1, 200)\n",
        "print(\"Theta after running gradient descent:\", theta)\n",
        "print(\"Resulting cost:\", costs[-1])"
      ],
      "metadata": {
        "id": "gR9Bfg4dCIXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 iteration 에 대한 cost 값 그리기\n",
        "\n",
        "plt.plot(costs)\n",
        "\n",
        "plt.title(\"Values of Cost Function over iterations of Gradient Descent\");\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"$J(\\Theta)$\")"
      ],
      "metadata": {
        "id": "HznBY9q-FgkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression algorithm 으로 학습된 decision boundary 그리기\n",
        "\n",
        "# 그려질 그래프의 두 축의 최대, 최소값 설정\n",
        "max_x = np.max(X[:, 0])\n",
        "min_x = np.min(X[:, 0])\n",
        "\n",
        "max_y = np.max(X[:, 1])\n",
        "min_y = np.min(X[:, 1])\n",
        "\n",
        "# Half moon 데이터 그리기\n",
        "plt.scatter(X[:, 0], X[:,1], \n",
        "            s=40, \n",
        "            c=y,\n",
        "            cmap=plt.cm.Spectral)\n",
        "\n",
        "\n",
        "# Decision boundary 그리기\n",
        "x_hyperplane = np.linspace(-2, 3, 10)\n",
        "y_hyperplane = -(theta[0] + theta[1] * x_hyperplane) / theta[2]\n",
        "\n",
        "plt.plot(x_hyperplane, y_hyperplane, '-')\n",
        "plt.title(\"Logistic regression algorithm on Half moon dataset\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "\n",
        "plt.xlim((min_x - 0.5, max_x + 0.5))\n",
        "plt.ylim((min_y - 0.5, max_y + 0.5))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XlqbII2vFggM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 계산된 최적의 가중치 (theta) 이용하여 예측 수행하기\n",
        "\n",
        "# 에측을 위한 함수 설정 (예측 자체는 linear classification 과 동일)\n",
        "def predict(theta, x):\n",
        "    results = x.dot(theta)\n",
        "    return results > 0\n",
        "\n",
        "# 주어진 데이터에 대한 예측 수행\n",
        "p = predict(theta, X_)\n",
        "print(\"Training Accuracy:\", sum(p==y)[0],\"%\")"
      ],
      "metadata": {
        "id": "Y9GAe3c8FgaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}